\documentclass{beamer}

\input{MathMacros}

% packages
\usepackage{beamerthemesplit}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage[all,cmtip]{xy}
\usepackage[mathscr]{eucal}
\usepackage[natbib=true,style=authoryear,backend=bibtex,useprefix=true]{biblatex}
\addbibresource{reading.bib}
\usepackage{graphicx,color}

\title{RL}
\author{Reuben Brasher}
\date{\today}

\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\section{Pictures of games}

\frame
{
   \frametitle{$k$-armed Bandit}
   
   %Refer to Fig. \ref{fig:narmed}.
   
   \begin{figure}[ht]
      \includegraphics[height=1.8in,keepaspectratio]{images/Slot\_machines\_at\_Wookey\_Hole\_Caves.JPG}
      \caption{From https://en.wikipedia.org/wiki/Slot\_machine} \label{fig:narmed}
   \end{figure}
}

\frame
{
   \frametitle{Markov Decision Process}
   
   %Refer to Fig. \ref{fig:mdp}.
   
   \begin{figure}[ht]
      \includegraphics[height=1.8in,keepaspectratio]{images/Reinforcement\_learning\_diagram.svg.png}
      \caption{From https://en.wikipedia.org/wiki/Reinforcement\_learning} \label{fig:mdp}
   \end{figure}
}

\section{\textit{k}-armed Bandits}

\frame
{
   \frametitle{Problem}

   Each round $t$ an agent may choose an action from $k$ possible. The agent
   receives a reward sampled from a distribution conditioned on the action.
   $$P(R_t|A_t)$$
   The objective of the game is to learn which action will give the highest
   expected reward.
}

\frame
{
   \frametitle{$q_*$}

   \begin{itemize}
      \item<1-> If only we knew the \textit{value} of each action

      $$q_*(a) = \mathbb{E} [R_t | A_t=a].$$

      \item<2-> We do not. We know $Q_t(a)$.

   \end{itemize}
}


\frame
{
   \frametitle{Explore vs. Exploit}

   \begin{itemize}
      \item<1-> Greedy strategy is always choose current best 
      $$\argmax_a Q_t(a)$$
      
      \item<2-> $\varepsilon$-Greedy strategy is to choose uniformly randomly
      probability $\varepsilon$, and to follow greedy strategy otherwise.
      
      \item<3-> Upper confidence bound strategy is to choose
      $$\argmax_a \sqbrak{Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}}$$
      
   \end{itemize}
}

\section{Reinforcement Learning}

\frame
{
   \frametitle{Finite Markov Decision Process}
   
   Agent and environment interact to produce a trajectory
   
   $$S_0,\,A_0,\,R_1,\,S_1,\,A_1,\,R_2,\,S_2,\,A_2,\,R_3,\dots$$
   
   State and reward depend on previous state and agent action
   
   $$p(s', r | s, a) = \Pr \prn{S_t=s', R_t=r | S_{t-1}=s,A_{t-a}=a}$$
}

\frame
{
   \frametitle{$G_t$}

   \begin{itemize}
      \item<1-> \textit{Return}
      $$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T$$

      \item<2-> \textit{Discounted return}
      $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum^\infty_{k=0} \gamma ^k R_{t+k+1}$$

   \end{itemize}
}

\frame
{
   \frametitle{Policy, value and action value function}
   
   Agent responds to environment by sampling from \textit{policy} $\pi(a|s)$.
   
   $Value$ of state $s$ is
   
   $$v_\pi (s) = \mathbb{E} [G_t|S_t = s]$$
   
   Action value function is
   $$q_\pi(s, a) = \mathbb{E} [G_t|S_t = s, A_t = a]$$ 
}

\frame
{
   \frametitle{If only we knew\dots}
  
   The \textit{optimal policy} $\pi_*$ or
   
   the \textit{optimal value} of state $s$
   
   $$v_* (s) = \max_\pi v_\pi(a)$$
   
   or the \textit{optimal action value function}
   
   $$q_*(s, a) = \max_\pi q_\pi(s, a)$$
   
   Note that
   $$q_*(s, a) = \mathbb{E} \sqbrak{R_{t+1} + \gamma v_* \prn{S_{t+1}} | S_t = s, A_t=a}$$ 
}

\frame
{
   \frametitle{Bellman optimality equations}
   
   \begin{itemize}
      \item<1-> Bellman equation is 
      $$v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r| s, a) \sqbrak{r + \gamma v_\pi(s')}$$
      
      \item<2-> Bellman optimality equation for value is
      $$v_*(s) = \max_a \sum_{s', r} p(s', r| s, a) [r + \gamma v_*(s')]$$
      
      \item<3-> Bellman optimality equation for action value is
      $$q_*(s,a) = \sum_{s', r} p(s', r| s, a) [r +  \gamma \max_{a'} q_*(s', a')]$$
      
   \end{itemize}
}

\frame
{
   \frametitle{Finding optimal policy}
   
   \begin{itemize}
      \item<1-> By policy iteration (pg 121)
      $$\pi_0 \to v_{\pi_0} \to \pi_1 \to v_{\pi_1} \to \cdots \to \pi_* \to v_*$$
      
      \item<2-> By computing $q_*$
      
      \item<3-> By computing $v_*$
      
   \end{itemize}
}

\frame
{
   \frametitle{Temporal Difference}
   Iteratively update $V$ to estimate $v_\pi$.   
   
   Update $V$ by
   $$V(S_t) \leftarrow V(S_t) + \alpha \sqbrak{G_t - V(S_t)}$$
   
   Why wait though?
   $$V(S_t) \leftarrow V(S_t) + \alpha \sqbrak{R_{t+1} + \gamma V(S_{t+1}) - V(S_t)}$$
}

\frame
{
   \frametitle{SARSA}
   Iteratively update $Q$ to estimate $q_*$. (pg 151)   
   
   Update $Q$ by
   $$Q(S_t,A_t) \leftarrow Q(S_t, A_t) + \alpha \sqbrak{R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t)}$$
}

\frame
{
   \frametitle{Q-learning}
   Iteratively update $Q$ to estimate $q_*$. (pg 153)   
   
   Update $Q$ by
   $$Q(S_t,A_t) \leftarrow Q(S_t, A_t) + \alpha \sqbrak{R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t,A_t)}$$
}

\frame
{
   \frametitle{Policy Gradient}
   Parameterize policy $\pi$ by $\theta$, for example
   $$\pi(a|s, \theta) = \frac{e^{h(s, a, \theta)}}{\sum_b e^{h(s, b, \theta)}}$$
   where $h(s, a, \theta)$ is ANN or similar.

   Then $\pi$ has a gradient with respect to $\theta$, and $v_\pi$ can be 
   improved by gradient ascent.
}

\frame
{
   \frametitle{Actor critic}
   Parameterize policy $\pi$ by $\theta$, and estimate $v$ by $w$
   $$\pi(a|s, \theta) = \frac{e^{h(s, a, \theta)}}{\sum_b e^{h(s, b, \theta)}}$$
   where $h(s, a, \theta)$ is ANN or and so is $v(s, w)$.

   Then $\pi$ has a gradient with respect to $\theta$, and $v$ with respect to
   $w$. Improve $v$ and $\pi$ in turn by applying gradient updates.
}

\frame
{
   \frametitle{Futher reading}
   \cite{sutton1998reinforcement}
   
   \cite{narasimhan2015language}
   
   \cite{li2016deep}
   
   \cite{mnih2015human}
   
   \cite{zhu2016target}
}

\begin{frame}[t,allowframebreaks]
\frametitle{References}
\printbibliography
\end{frame}

\end{document}
