\documentclass{beamer}

\input{MathMacros}

% packages
\usepackage{beamerthemesplit}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage[all,cmtip]{xy}
\usepackage[mathscr]{eucal}
\usepackage[natbib=true,style=authoryear,backend=bibtex,useprefix=true]{biblatex}
\addbibresource{reading.bib}
\usepackage{graphicx,color}

\title{RL}
\author{Reuben Brasher}
\date{\today}

\begin{document}

\frame{\titlepage}

\section[Outline]{}
\frame{\tableofcontents}

\section{Pictures of games}

\frame
{
   \frametitle{$k$-armed Bandit}
   
   %Refer to Fig. \ref{fig:narmed}.
   
   \begin{figure}[ht]
      \includegraphics[height=1.8in,keepaspectratio]{images/Slot\_machines\_at\_Wookey\_Hole\_Caves.JPG}
      \caption{From https://en.wikipedia.org/wiki/Slot\_machine} \label{fig:narmed}
   \end{figure}
}

\frame
{
   \frametitle{Markov Decision Process}
   
   %Refer to Fig. \ref{fig:mdp}.
   
   \begin{figure}[ht]
      \includegraphics[height=1.8in,keepaspectratio]{images/Reinforcement\_learning\_diagram.svg.png}
      \caption{From https://en.wikipedia.org/wiki/Reinforcement\_learning} \label{fig:mdp}
   \end{figure}
}

\section{$k$-armed Bandits}

\frame
{
   \frametitle{Problem}

   Each round $t$ an agent may choose an action from $k$ possible. The agent
   receives a reward sampled from a distribution conditioned on the action.
   $$P(R_t|A_t)$$
   The objective of the game is to learn which action will give the highest
   expected reward.
}

\frame
{
   \frametitle{$q_*$}

   \begin{itemize}
      \item<1-> If only we knew the \textit{value} of each action

      $$q_*(a) = \mathbb{E} [R_t | A_t=a].$$

      \item<2-> We do not. We know $Q_t(a)$.

   \end{itemize}
}


\frame
{
   \frametitle{Explore vs. Exploit}

   \begin{itemize}
      \item<1-> Greedy strategy is always choose current best 
      $$\argmax_a Q_t(a)$$
      
      \item<2-> $\varepsilon$-Greedy strategy is to choose uniformly randomly
      probability $\varepsilon$, and to follow greedy strategy otherwise.
      
      \item<3-> Upper confidence bound strategy is to choose
      $$\argmax_a \sqbrak{Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}}$$
      
   \end{itemize}
}

\section{Reinforcement Learning}

\frame
{
   \frametitle{Finite Markov Decision Process}
   
   Agent and environment interact to produce a trajectory
   
   $$S_0,\,A_0,\,R_1,\,S_1,\,A_1,\,R_2,\,S_2,\,A_2,\,R_3,\dots$$
   
   State and reward depend on previous state and agent action
   
   $$p(s', r | s, a) = \Pr \prn{S_t=s', R_t=r | S_{t-1}=s,A_{t-a}=a}$$
}

\frame
{
   \frametitle{Attention Layers}

   \begin{itemize}
      \item<1-> Let $x_j$ be the input sequence and $h_j$ encoding by RNN.

      \item<2-> Let $y_i$ be the target sequence, and $s_i$ a hidden state.

      \item<3-> $$s_i = f \prn{s_{i-1}, y_{i-1}, c_i}$$

      \item<4-> $c_i$, called the context vector is

      $$c_i = \sum_j \alpha_{ij} h_j$$

      \item<5-> $\alpha_{ij}$ is the importance of $h_j$ for $s_i$

      $$\alpha_{ij} = \frac{\exp \prn{e_{ij}}} {\sum_k \exp \prn{e_{ik}}}$$

      where $e_{ij} = a \prn{a_{i-1}, h_j}$.

      
   \end{itemize}
}

\begin{frame}[t,allowframebreaks]
   \frametitle{References}
   \printbibliography
\end{frame}

\end{document}
